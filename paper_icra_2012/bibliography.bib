%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Lucas Chavez at 2012-09-16 20:30:29 -0300 


%% Saved with string encoding Unicode (UTF-8) 



@article{henry2012rgb,
	Author = {Henry, P. and Krainin, M. and Herbst, E. and Ren, X. and Fox, D.},
	Date-Added = {2012-09-16 20:07:35 -0300},
	Date-Modified = {2012-09-16 20:07:35 -0300},
	Journal = {IJRR},
	Publisher = {SAGE Publications},
	Title = {RGB-D mapping: Using Kinect-style depth cameras for dense 3D modeling of indoor environments},
	Year = {2012}}

@article{newman2009navigating,
	Author = {Newman, P. and Sibley, G. and Smith, M. and Cummins, M. and Harrison, A. and Mei, C. and Posner, I. and Shade, R. and Schroeter, D. and Murphy, L. and others},
	Date-Added = {2012-09-16 20:00:08 -0300},
	Date-Modified = {2012-09-16 20:00:08 -0300},
	Journal = {IJRR},
	Number = {11-12},
	Pages = {1406--1433},
	Publisher = {SAGE Publications},
	Title = {Navigating, recognizing, and describing urban spaces with vision and lasers},
	Volume = {28},
	Year = {2009}}

@article{may2009three,
	Author = {May, S. and Droeschel, D. and Holz, D. and Fuchs, S. and Malis, E. and N{\"u}chter, A. and Hertzberg, J.},
	Date-Added = {2012-09-16 19:56:18 -0300},
	Date-Modified = {2012-09-16 19:56:18 -0300},
	Journal = {Journal of field robotics},
	Number = {11-12},
	Pages = {934--965},
	Publisher = {Wiley Online Library},
	Title = {Three-dimensional mapping with time-of-flight cameras},
	Volume = {26},
	Year = {2009}}

@inproceedings{clemente2007mapping,
	Author = {Clemente, L.A. and Davison, A.J. and Reid, I. and Neira, J. and Tard{\'o}s, J.D.},
	Booktitle = {Robotics: Science and Systems},
	Date-Added = {2012-09-16 19:49:25 -0300},
	Date-Modified = {2012-09-16 19:49:25 -0300},
	Title = {Mapping large loops with a single hand-held camera},
	Year = {2007}}

@article{kaess2008isam,
	Author = {Kaess, M. and Ranganathan, A. and Dellaert, F.},
	Date-Added = {2012-09-16 19:07:45 -0300},
	Date-Modified = {2012-09-16 19:07:45 -0300},
	Journal = {Robotics, IEEE Transactions on},
	Number = {6},
	Pages = {1365--1378},
	Publisher = {IEEE},
	Title = {iSAM: Incremental smoothing and mapping},
	Volume = {24},
	Year = {2008}}

@article{durrant2006simultaneous,
	Author = {Durrant-Whyte, H. and Bailey, T.},
	Date-Added = {2012-09-16 19:06:54 -0300},
	Date-Modified = {2012-09-16 19:06:54 -0300},
	Journal = {Robotics and Automation Magazine},
	Number = {99},
	Pages = {80},
	Publisher = {Citeseer},
	Title = {Simultaneous localisation and mapping (SLAM): Part I the essential algorithms},
	Volume = {13},
	Year = {2006}}

@article{smith1990estimating,
	Author = {Smith, R. and Self, M. and Cheeseman, P.},
	Date-Added = {2012-09-16 19:05:01 -0300},
	Date-Modified = {2012-09-16 19:05:01 -0300},
	Journal = {Autonomous robot vehicles},
	Pages = {167--193},
	Title = {Estimating uncertain spatial relationships in robotics},
	Volume = {1},
	Year = {1990}}

@book{Thrun:2005:PR:1121596,
	Author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
	Date-Added = {2012-09-07 16:59:04 -0300},
	Date-Modified = {2012-09-07 16:59:04 -0300},
	Isbn = {0262201623},
	Publisher = {The MIT Press},
	Title = {Probabilistic Robotics (Intelligent Robotics and Autonomous Agents)},
	Year = {2005}}

@inproceedings{Hahnel2004,
	Abstract = {We analyze whether radio frequency identification (RFID) technology can be used to improve the localization of mobile robots and persons in their environment. In particular we study the problem of localizing RFID tags with a mobile platform that is equipped with a pair of RFID antennas. We present a probabilistic measurement model for RFID readers that allow us to accurately localize RFID tags in the environment. We also demonstrate how such maps can be used to localize a robot and persons in their environment. Finally, we present experiments illustrating that the computational requirements for global robot localization can be reduced strongly by fusing RFID information with laser data.},
	Author = {Hahnel, D and Burgard, W and Fox, D and Fishkin, K and Philipose, M},
	Booktitle = {IEEE International Conference on Robotics and Automation 2004 Proceedings ICRA 04 2004},
	Doi = {10.1109/ROBOT.2004.1307283},
	File = {:Users/jamesmilligan/Documents/Research Library/Hahnel et al. - 2004 - Mapping and localization with RFID technology.pdf:pdf},
	Isbn = {0780382323},
	Issn = {10504729},
	Number = {6},
	Organization = {Intel Research},
	Pages = {1015--1020},
	Publisher = {Ieee},
	Title = {{Mapping and localization with RFID technology}},
	Volume = {1},
	Year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ROBOT.2004.1307283}}

@inproceedings{Borenstein95,
	Author = {Johann Borenstein and Liqiang Feng},
	Booktitle = {IROS '95},
	Pages = {569-574},
	Title = {Correction of Systematic Odometry Errors in Mobile Robots},
	Year = {1995}}

@inproceedings{Wu07,
	Author = {Jing Wu and Hong Zhang},
	Booktitle = {Fourth Canadian Conference on Computer and Robot Vision, 2007. CRV '07.},
	Month = {may},
	Pages = {149 -156},
	Title = {Camera Sensor Model for Visual SLAM},
	Year = {2007}}

@incollection{Levinson2007,
	Abstract = {Many urban navigation applications (e.g., autonomous navigation, driver assistance systems) can benefit greatly from localization with centimeter accuracy. Yet such accuracy cannot be achieved reliably with GPS-based inertial guidance systems, specifically in urban settings. We propose a technique for high-accuracy localization of moving vehicles that utilizes maps of urban environments. Our approach integrates GPS, IMU, wheel odometry, and LIDAR data acquired by an instrumented vehicle, to generate high-resolution environment maps. Offline relaxation techniques similar to recent SLAM methods 2, 10, 13, 14, 21, 30 are employed to bring the map into alignment at intersections and other regions of self-overlap. By reducing the final map to the flat road surface, imprints of other vehicles are removed. The result is a 2-D surface image of ground reflectivity in the infrared spectrum with 5cm pixel resolution. To localize a moving vehicle relative to these maps, we present a particle filter method for correlating LIDAR measurements with this map. As we show by experimentation, the resulting relative accuracies exceed that of conventional GPS-IMU-odometry-based methods by more than an order of magnitude. Specifically, we show that our algorithm is effective in urban environments, achieving reliable real-time localization with accuracy in the 10-centimeter range. Experimental results are provided for localization in GPS-denied environments, during bad weather, and in dense traffic.},
	Author = {Levinson, Jesse and Montemerlo, Michael and Thrun, Sebastian},
	Booktitle = {Robotics: Science and Systems III},
	Editor = {Burgard, Wolfram and Brock, Oliver and Stachniss, Cyrill},
	File = {:Users/jamesmilligan/Documents/Research Library/Levinson, Montemerlo, Thrun - 2007 - Map-Based Precision Vehicle Localization in Urban Environments.pdf:pdf},
	Isbn = {978-0-262-52484-1},
	Publisher = {The MIT Press},
	Title = {{Map-Based Precision Vehicle Localization in Urban Environments}},
	Year = {2007}}

@inproceedings{Se2001,
	Author = {Se, S and Lowe, D and Little, J},
	Booktitle = {Robotics and Automation, 2001. Proceedings 2001 ICRA. IEEE International Conference on},
	Doi = {10.1109/ROBOT.2001.932909},
	File = {:Users/jamesmilligan/Documents/Research Library/Se, Lowe, Little - 2001 - Vision-based mobile robot localization and mapping using scale-invariant features.pdf:pdf},
	Isbn = {1050-4729 VO - 2},
	Keywords = {3D landmarks,3D map,Triclops stereo vision system,feature viewpoint variation,landmarks,mobile robots,motion estimation,position measurement,robot ego-motion,robot pose estimation,robot vision,scale-invariant image features,stereo image processing,vision-based mobile robot localization,vision-based mobile robot mapping},
	Pages = {2051--2058 vol.2},
	Title = {{Vision-based mobile robot localization and mapping using scale-invariant features}},
	Volume = {2},
	Year = {2001},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ROBOT.2001.932909}}

@inproceedings{Wavering1993,
	Author = {Wavering, A J and Fiala, J C and Roberts, K J and Lumia, R},
	Booktitle = {Robotics and Automation, 1993. Proceedings., 1993 IEEE International Conference on},
	Doi = {10.1109/ROBOT.1993.292207},
	File = {:Users/jamesmilligan/Documents/Research Library/Wavering et al. - 1993 - TRICLOPS a high-performance trinocular active vision system.pdf:pdf},
	Isbn = {VO -},
	Keywords = {CCD image sensors,TRICLOPS,cameras,center wide-angle view camera,computer vision,dynamic performance,four-degree-of-freedom direct-drive system,high-performance,image processing equipment,intelligent control,intelligently controlled,multiresolution trinocular camera-pointing system,optical positioning system,position control,robotic vision,telecontrol,telerobotics,tracking,trinocular active vision system,vergence cameras,visual tracking},
	Pages = {410--417 vol.3},
	Title = {{TRICLOPS: a high-performance trinocular active vision system}},
	Year = {1993},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ROBOT.1993.292207}}

@patent{Freedman2008,
	Abstract = {Apparatus for mapping an object includes an illumination assembly(30, 50), which includes a single transparency (36) containing a fixed pattern (60, 70, 80) of spots. A light source (34, 52) transilluminates the single transparency with optical radiation so as to project the pattern onto the object (28). An image capture assembly (32) captures an image of the pattern that is projected onto the object using the single transparency. A processor (24) processes the image captured by the image capture assembly so as to reconstruct a three-dimensional (3D) map of the object.},
	Annote = {patent for the kinect sensor technology},
	Author = {Freedman, Barak and Shpunt, Alexander and Machline, Meir and Arieli, Yoel},
	Day = {9},
	File = {:Users/jamesmilligan/Documents/Research Library/Freedman et al. - 2008 - Depth mapping using projected patterns.pdf:pdf},
	Month = oct,
	Number = {WO 2008/120217 A2},
	Title = {{Depth mapping using projected patterns}},
	Type = {Patent Application},
	Year = {2008}}

@inproceedings{Izadi2011,
	Abstract = {KinectFusion enables a user holding and moving a standard Kinect camera to rapidly create detailed 3D reconstructions of an indoor scene. Only the depth data from Kinect is used to track the 3D pose of the sensor and reconstruct, geomet- rically precise, 3D models of the physical scene in real-time. The capabilities of KinectFusion, as well as the novel GPU- based pipeline are described in full. We showuses of the core system for low-cost handheld scanning, and geometry-aware augmented reality and physics-based interactions. Novel ex- tensions to the core GPU pipeline demonstrate object seg- mentation and user interaction directly in front of the sensor, without degrading camera tracking or reconstruction. These extensions are used to enable real-time multi-touch interac- tions anywhere, allowing any planar or non-planar recon- structed physical surface to be appropriated for touch.},
	Address = {Santa Barbara, CA},
	Author = {Izadi, Shahram and Kim, David and Hilliges, Otmar and Molyneaux, David and Newcombe, Richard and Kohli, Pushmeet and Shotton, Jamie and Hodges, Steve and Freeman, Dustin and Davison, Andrew and Fitzgibbon, Andrew},
	Booktitle = {ACM Symposium on User Interface Software and Technology},
	File = {:Users/jamesmilligan/Documents/Research Library/Izadi et al. - 2011 - KinectFusion Real-time 3D Reconstruction and Interaction Using a Moving Depth Camera.pdf:pdf},
	Isbn = {9781450307161},
	Title = {{KinectFusion : Real-time 3D Reconstruction and Interaction Using a Moving Depth Camera}},
	Year = {2011}}

@inproceedings{Xia2011,
	Abstract = {Conventional human detection is mostly done in images taken by visible-light cameras. These methods imitate the detection process that human use. They use features based on gradients, such as histograms of oriented gradients (HOG), or extract interest points in the image, such as scale-invariant feature transform (SIFT), etc. In this paper, we present a novel human detection method using depth information taken by the Kinect for Xbox 360. We propose a model based approach, which detects humans using a 2-D head contour model and a 3-D head surface model. We propose a segmentation scheme to segment the human from his/her surroundings and extract the whole contours of the figure based on our detection point. We also explore the tracking algorithm based on our detection result. The methods are tested on our database taken by the Kinect in our lab and present superior results.},
	Address = {Colorado Springs, CO},
	Author = {Xia, Lu and Chen, Chia-chih and Aggarwal, J K},
	Booktitle = {International Workshop on Human Activity Understanding from 3D Data},
	Date-Modified = {2012-09-07 16:55:59 -0300},
	File = {:Users/jamesmilligan/Documents/Research Library/Xia, Chen, Aggarwal - 2011 - Human Detection Using Depth Information by Kinect Department of Electrical and Computer Engineering.pdf:pdf},
	Title = {{Human Detection Using Depth Information by Kinect}},
	Year = {2011}}

@article{Stowers2011,
	Abstract = {Reliable depth estimation is a cornerstone of many autonomous robotic control systems. The Microsoft Kinect is a new, low cost, commodity game controller peripheral that calculates a depth map of the environment with good accuracy and high rate. In this paper we calibrate the Kinect depth and image sensors and then use the depth map to control the altitude of a quadrotor helicopter. This paper presents the first results of using this sensor in a real-time robotics control application.},
	Author = {Stowers, John and Hayes, Michael and Bainbridge-Smith, Andrew},
	Doi = {10.1109/ICMECH.2011.5971311},
	File = {:Users/jamesmilligan/Documents/Research Library/Stowers, Hayes, Bainbridge-Smith - 2011 - Altitude control of a quadrotor helicopter using depth map from Microsoft Kinect sensor.pdf:pdf},
	Isbn = {978-1-61284-982-9},
	Journal = {2011 IEEE International Conference on Mechatronics},
	Keywords = {depth map,microsoft kinect,quadrotor,visul flight control},
	Month = apr,
	Pages = {358--362},
	Publisher = {Ieee},
	Title = {{Altitude control of a quadrotor helicopter using depth map from Microsoft Kinect sensor}},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICMECH.2011.5971311}}

@misc{ROS_kinect,
	Month = aug,
	Title = {{Technical description of Kinect calibration}},
	Url = {http://www.ros.org/wiki/kinect\_calibration/technical},
	Urldate = {August 13, 2012},
	Year = {2012},
	Bdsk-Url-1 = {http://www.ros.org/wiki/kinect%5C_calibration/technical}}

@article{Kahlmann2006,
	Abstract = {High-altitude headache often fulfills the criteria of migraine. Therefore, we hypothesized that sumatriptan, a 5-HT1 receptor agonist specifically effective for treatment of migraine, would also alleviate high altitude headache. A randomized, placebo-controlled double-blind trial was performed on 29 mountaineers with at least moderate headache on the day of arrival at 4559 m. Fourteen subjects received 100 mg sumatriptan orally and 15 subjects received placebo. Before treatment there were no significant differences between groups regarding rate of ascent, duration and severity of headache, and acute mountain sickness score. All 6 female subjects were randomly assigned to placebo. Absolute values and the reduction of headache scores 1, 3, and 12 h after the administration of sumatriptan did not differ between treatment groups, but headache scores tended to be lower with sumatriptan after 1 or 3 h when compared with placebo. Considering only male mountaineers, there was a significant decrease of headache scores after 1 and 3 h. Because there was only a minor transient amelioration of high altitude headache with sumatriptan, we conclude that 5-HT1 receptors do not play a major role in the pathophysiology of high altitude headache.},
	Author = {Kahlmann, T and Remondino, F and Ingensand, H},
	Doi = {10.1089/15270290260512864},
	Editor = {Maas, H G and Schneider, D},
	File = {:Users/jamesmilligan/Documents/Research Library/Kahlmann, Remondino, Ingensand - 2006 - Calibration for increased accuracy of the range imaging camera swissrangertm.pdf:pdf},
	Journal = {Proc of IEVM},
	Keywords = {accuracy,calibration,camera,flash ladar,laser scanning,lidar,measurement system,range imaging},
	Number = {4},
	Pages = {136--141},
	Pmid = {12631424},
	Publisher = {Citeseer},
	Title = {{Calibration for increased accuracy of the range imaging camera swissranger\texttrademark}},
	Volume = {36},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1089/15270290260512864}}

@inproceedings{Chiabrando2009,
	Abstract = {3D imaging with Time-of-Flight (ToF) cameras is a promising recent technique which allows 3D point clouds to be acquired at video frame rates. However, the distance measurements of these devices are often affected by some systematic errors which decrease the quality of the acquired data. In order to evaluate these errors, some experimental tests on a CCD/CMOS ToF camera sensor, the SwissRanger (SR)-4000 camera, were performed and reported in this paper. In particular, two main aspects are treated: the calibration of the distance measurements of the SR-4000 camera, which deals with evaluation of the camera warm up time period, the distance measurement error evaluation and a study of the influence on distance measurements of the camera orientation with respect to the observed object; the second aspect concerns the photogrammetric calibration of the amplitude images delivered by the camera using a purpose-built multi-resolution field made of high contrast targets.},
	Author = {Chiabrando, Filiberto and Chiabrando, Roberto and Piatti, Dario and Rinaudo, Fulvio},
	Booktitle = {Sensors (Basel, Switzerland)},
	Doi = {10.3390/s91210080},
	File = {:Users/jamesmilligan/Documents/Research Library/Chiabrando et al. - 2009 - Sensors for 3D Imaging Metric Evaluation and Calibration of a CCDCMOS Time-of-Flight Camera.pdf:pdf},
	Issn = {1424-8220},
	Keywords = {calibration,ccd,cmos sensor,range imaging,systematic errors,time flight camera},
	Month = jan,
	Number = {12},
	Pages = {10080--96},
	Pmid = {22303163},
	Title = {{Sensors for 3D Imaging: Metric Evaluation and Calibration of a CCD/CMOS Time-of-Flight Camera.}},
	Volume = {9},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.3390/s91210080}}

@inproceedings{Fallon2012,
	Author = {Fallon, M F and Johannsson, H and Leonard, J J},
	Booktitle = {ICRA 2012},
	Doi = {10.1109/ICRA.2012.6224951},
	File = {:Users/jamesmilligan/Documents/Research Library/Fallon, Johannsson, Leonard - 2012 - Efficient scene simulation for robust monte carlo localization using an RGB-D camera.pdf:pdf},
	Isbn = {1050-4729 VO -},
	Keywords = {3D information,GPU,KMCL algorithm,Kinect Monte Carlo localization,Microsoft Kinect,Monte Carlo methods,RGB-D camera,RWI B21 wheeled mobile robot platform,a priori 3-D model,ascending technologies quadrotor,efficient scene simulation,feature-based visual odometry,graphical processing unit,graphics processing units,image colour analysis,large planar segments,mobile robots,particle propagation mechanism,pure distance-based method,robot vision,robotic wheelchair,robust Monte Carlo localization,sensor image formation model,solid modelling,three dimensional indoor environments,wearable computing,willow garage PR2},
	Pages = {1663--1670},
	Title = {{Efficient scene simulation for robust monte carlo localization using an RGB-D camera}},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICRA.2012.6224951}}

@inproceedings{Newcombe2011,
	Abstract = {We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point (ICP) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and GPU hardware, promises an exciting step forward in augmented reality (AR), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision.},
	Author = {Newcombe, Richard A and Izadi, Shahram and Hilliges, Otmar and Molyneaux, David and Kim, David and Davison, Andrew J and Kohli, Pushmeet and Shotton, Jamie and Hodges, Steve and Fitzgibbon, Andrew},
	Booktitle = {IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2011.6092378},
	File = {:Users/jamesmilligan/Documents/Research Library/Newcombe et al. - 2011 - KinectFusion Real-time dense surface mapping and tracking.pdf:pdf},
	Isbn = {9781457721847},
	Keywords = {ar,dense reconstruction,depth cameras,gpu,real time,slam,tracking,volumetric representation},
	Number = {10},
	Organization = {Imperial College London, UK},
	Pages = {127--136},
	Publisher = {IEEE},
	Title = {{KinectFusion: Real-time dense surface mapping and tracking}},
	Volume = {7},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2011.6092378}}

@article{Elfes1989,
	Address = {Los Alamitos, CA, USA},
	Author = {Alberto Elfes},
	Doi = {http://doi.ieeecomputersociety.org/10.1109/2.30720},
	Issn = {0018-9162},
	Journal = {Computer Magazine, Special Issue on Autonomous Intelligent Machines},
	Pages = {46-57},
	Publisher = {IEEE Computer Society},
	Title = {Using Occupancy Grids for Mobile Robot Perception and Navigation},
	Volume = {22},
	Year = {1989},
	Bdsk-Url-1 = {http://doi.ieeecomputersociety.org/10.1109/2.30720}}
