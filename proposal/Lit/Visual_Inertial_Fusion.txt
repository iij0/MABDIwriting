Real-Time Metric State Estimation for Modular Vision-Inertial Systems
 ETH Zurich - Stephan Weiss and Roland Siegwart
- Uses an Extended Kalman Filter to fuse IMU and monocular camera information to estimate the state of a camera-IMU device. The state is the position and velocity of the system.
- The approach is modular and should be able to accommodate any system which can return either a 3 DOF or 6 DOF estimate of the sensor movement.
- They do a great job of explaining the setup of the Kalman Filter 

Versatile Distributed Pose Estimation and Sensor Self-Calibration for an Autonomous MAV
 ETH Zurich - Stephen Weiss, Markus W. Achtelik, Margarita Chili, Roland Siegwart
- Paper describes the implementation of the algorithm described in their prior paper. “Real-Time Metric State Estimation for Modular Vision-Inertial Systems” (see above)
- Re Summarizes the EKF algorithm and describes the measurement covariances they used. This is extremely important because that is the “black magic” behind any Kalman FIlter implementation. 
- Describe how to deal with different update rates in the filter
- Results show they were able to reliably obtain 6DOF pose estimation with their system. 

Robust Real-Time Visual Odometry with a Single Camera and an IMU
 ETH Zurich - Laurent Kneip, Margarita Chli, Roland Siegwart
- Utilizes frames taken at different times to obtain stereo features which are used for Visual Odometry (VO)
- The pose is obtained from the features via structure from motion.
- The IMU is used to determine the rotational delta from keyframe-to-keyframe. This creates a more robust computation.
- They break down the mechanics of how the rotational information is used. Could be useful for setting up equations 

Vision and IMU Data Fusion: Closed-Form Solutions for Attitude, Speed, Absolute Scale and Bias Determination 
 INRIA Rhone Alpes, France - e-Motion Lab - Agostino Martinelli
- Derives the observability of a vision and inertial data fusion system. “Find all physical quantities that the information contained in the sensor data allows us to estimate”
- Describes a step 2. “Find a reliable and efficient method to estimate these physical quantities” 
- In the literature review he mentions that the absolute roll and pitch angles are observable while the yaw angle is unobservable 
- One of his “deltas” is that he is calculating the minimum number of camera observations needed. 
- Derived a closed form solution to calculate the observable modes from the sensor data. Pretty impressive statement.
- Tested his work on real world data. (IMU and camera on a quadrotor) 

Deterministic Initialization of Metric State Estimation Filters for Loosely-Coupled Monocular Vision-Inertial Systems
 ETH Zurich Autonomous Systems Lab - Laurent Kneip, Stephan Weiss, and Roland Siegwart
- They mention advances in deterministic scale computation approaches (including Martinelli). They say the drawback of these methods is that they require double integration of the acceleration data. (we already know this is sketchy) 
- “The loosely-coupled approach treats the inertial and vision units as two separate filters running at different rates and exchanging information, while the tightly-coupled approach combines them into a single, statistical filter.”
- They claim their method is better because it is closed-form and only requires a single integration period
- They show a good work up of the math of the system 
- They describe a method to guard against noise during the integration step. Not sure exactly how it works but I believe it checks for agreement between the current integration result and results from the past. Maybe worth re-checking on
- The only calculate the pitch, roll, and scale factor. No translations 

Closed-Form Solution for Attitude and Speed Determination by Fusing Monocular Vision and Inertial Sensor Measurements
 INRIA Rhone Alpes, France - e-Motion Lab - Agostino Martinelli - accepted for presentation at ICRA 2011

State Estimation Based on the COncept of Continuous Symmetry and Observability Analysis: the Case of Calibration
 INRIA Rhone Alpes, France - e-Motion Lab - Agostino Martinelli - Transactions on Robotics Vol. 27, April  2011

Visual Odometry and Mapping for Autonomous Flight Using an RGB-D Camera

Gyro-aided feature tracking for a moving camera: fusion, auto-calibration and GPU implementation

An Introduction to Inertial and Visual Sensing

Fast Ego-motion Estimation with Multi-rate Fusion of Inertial and Vision

_________________________________________ Calibration of fused inertial and vision systems 

A Kalman Filter-based Algorithm for IMU-Camera Calibration: Observability Analysis and Performance Evaluation

Modeling and Calibration of Inertial and Vision Sensors

Relative pose calibration between visual and inertial sensors (they say it’s a toolkit?) (people seem to use this one)